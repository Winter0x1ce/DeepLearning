import torch

#---------------------入门-------------------------

# 张量表示一个由数值组成的数组，这个数组可能有多个维度。 
# 具有一个轴的张量对应数学上的向量（vector）； 
# 具有两个轴的张量对应数学上的矩阵（matrix）。

# 创建一个行向量x
x = torch.arange(12)

# 输出行向量内部内容
print(f"x = {x}")

# 访问张量（沿每个轴的长度）的形状 。
print(f"x.shape = {x.shape}")

# 输出张量的元素总数
print(x.numel())

# 改变张量形状，将x变为3x4的矩阵
X = x.reshape(3,4)
print(f"X = {X}")
print(f"X.shape = {X.shape}")

# 我们可以创建一个形状为（2,3,4）的张量，其中所有元素都设置为0。
x = torch.zeros((2,3,4))
print(f"x = {x}")
print(f"x.shape = {x.shape}")

# 同样，我们可以创建一个形状为(2,3,4)的张量，其中所有元素都设置为1。
x = torch.ones((2,3,4))
print(f"x = {x}")
print(f"x.shape = {x.shape}")

# 以下代码创建一个形状为（3,4）的张量。 
# 其中的每个元素都从均值为0、标准差为1的标准高斯分布（正态分布）中随机采样。
x = torch.randn(3,4)
print(f"x = {x}")

# 我们还可以通过提供包含数值的Python列表，来为所需张量中的每个元素赋予确定值。 
x = torch.tensor([[1,3,5,3],[4,3,2,5],[56,4,5,1]])
print(f"x = {x}")



#---------------------运算符------------------------
# 见的标准算术运算符（+、-、*、/和**）（**求幂运算）都可以被升级为按元素运算。
x = torch.tensor([1.0, 2, 4, 8])
y = torch.tensor([2, 2, 2, 2])
print(f"x+y = {x+y}")
print(f"x-y = {x-y}")
print(f"x*y = {x*y}")
print(f"x/y = {x/y}")
print(f"x**y = {x**y}")
# “按元素”方式可以应用更多的计算，包括像求幂这样的一元运算符。
x = torch.exp(x)
print(f"exp(x) = {x}")

# 我们也可以把多个张量连结（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量
# 下面的例子分别演示了当我们沿行（轴-0，形状的第一个元素） 和按列（轴-1，形状的第二个元素）连结两个矩阵时，会发生什么情况。

X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])
Z = torch.cat((X,Y),dim=0)
V = torch.cat((X,Y),dim=1)
print(f"Z = {Z}")
print(f"V = {V}")

# == 如果X和Y在该位置相等，则新张量中相应项的值为1,否则为0
print(f"X == Y : {X == Y}")


#---------------------广播机制------------------------
# 某些情况下，即使形状不同，我们仍然可以通过调用 广播机制（broadcasting mechanism）来执行按元素操作。
# 这种机制的工作方式如下：
# 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。
# 对生成的数组执行按元素操作。

a = torch.arange(3).reshape((3,1))
b = torch.arange(2).reshape((1,2))
print(f"a = {a}")
print(f"b = {b}")
c = a+b
print(f"a + b ={c}")


#---------------------索引和切片------------------------
# 就像在任何其他Python数组中一样，张量中的元素可以通过索引访问。
# 与任何Python数组一样：第一个元素的索引是0，最后一个元素索引是-1；
# 可以指定范围以包含第一个元素和最后一个之前的元素。
# 可以用[1:3]选择第二个和第三个元素，因为是从0计数，右边是开区间
x = torch.arange(16).reshape(4,4)
print(f"x = {x}")

print(f"x[-1] = {x[-1]}")
print(f"x[1:3] = {x[1:3]}")

# 除读取外，我们还可以通过指定索引来将元素写入矩阵。
x[1,2] = 9
print(f"x = {x}")

# 如果我们想为多个元素赋值相同的值，我们只需要索引所有元素，然后为它们赋值。
# 例如，[0:2, :]访问第1行和第2行，其中“:”代表沿轴1（列）的所有元素。
x[0:2,:] = 12
print(f'x = {x}')


#---------------------节省内存------------------------
# 运行一些操作可能会导致为新结果分配内存。 
# 例如，如果我们用Y = X + Y，我们将取消引用Y指向的张量，而是指向新分配的内存处的张量。
# 运行Y = Y + X后，我们会发现id(Y)指向另一个位置。 
# 这是因为Python首先计算Y + X，为结果分配新的内存，然后使Y指向内存中的这个新位置。
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])
before = id(Y)
Y = Y + X
id(Y) == before
print(f"id(Y) == before : {id(Y) == before}")

# 这可能是不可取的
# 首先，我们不想总是不必要地分配内存。在机器学习中，我们在一秒内多次更新所有参数。通常情况下，我们希望原地执行这些更新；
# 如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数。
# 我们可以使用切片表示法将操作的结果分配给先前分配的数组，例如Y[:] = <expression>。
Z = torch.zeros_like(Y)
print(f"id(Z): {id(Z)}")
Z[:] = X + Y
print('id(Z):',id(Z))


#---------------------转换为其他Python对象------------------------
# 将深度学习框架定义的张量转换为NumPy张量（ndarray）很容易，反之也同样容易。 
# torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。 
X = torch.arange(12, dtype=torch.float32).reshape((3,4))
Y = torch.tensor([[2.0,1,4,3],[1,2,3,4],[4,3,2,1]])
A = X.numpy()
B = torch.tensor(A)
print(f"type(A): {type(A)}")
print(f"type(B): {type(B)}")

# 要将大小为1的张量转换为Python标量，我们可以调用item函数或Python的内置函数。
a = torch.tensor([3.5])
